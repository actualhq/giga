{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47fbc2d-28c1-4284-a63c-aa85ac29e617",
   "metadata": {},
   "source": [
    "# Giga Dev Documentation\n",
    "\n",
    "This documentation provides technical details on the Giga model's implementation,\n",
    "setup, and deployment process.\n",
    "\n",
    "> Also see the following additional documentation:\n",
    "> * [User overview](main.ipynb) and details on running each notebook.\n",
    "> * [Model overview](models.ipynb), including a breakdown of each model.\n",
    "> * [Model data](data.ipynb), including data schemas and how to update countries.\n",
    "> * [Model architecture](arch.ipynb), focusing on key parts of the library used for configuration, data aggregation, and model execution.\n",
    "> * [Python documentation](../dev/documentation.ipynb) automatically generated from the model source code.\n",
    "\n",
    "### Repository Structure\n",
    "\n",
    "The python library in this repository is organized into the following key categories to help manage the models and their parameters:\n",
    "\n",
    "1. Models: the key building blocks of all computations performed by this library\n",
    "2. Schemas: the definitions of all the model inputs and outputs, data requirements, and configurations\n",
    "3. Data: the tooling to pull in and transform any external data into formats usable by the library\n",
    "4. Utilities: helpers for connecting to APIs, visualizing outputs, and constructing inspect able and interactive interfaces\n",
    "5. App: the application runner for configuring and starting the modeling application\n",
    "\n",
    "## Setup\n",
    "\n",
    "Note: this repositroy uses git lfs for some of the larger files.\n",
    "Please install [git lfs](https://git-lfs.com/), and then run `git lfs pull` to fetch copies of the larger files locally.\n",
    "Use [poetry](https://python-poetry.org/) to create a local development environment.\n",
    "Poetry is a tool for dependency management in Python.\n",
    "You can use the helper `dev` CLI to build the environment locally:\n",
    "\n",
    "```bash\n",
    "./dev build\n",
    "```\n",
    "\n",
    "To start a local notebook server simply run:\n",
    "\n",
    "```bash\n",
    "./dev start-notebook\n",
    "```\n",
    "\n",
    "You can use the `dev` CLI to also run pytest tests:\n",
    "\n",
    "```bash\n",
    "./dev test\n",
    "```\n",
    "\n",
    "### Lint\n",
    "\n",
    "You can format local code using the following commands:\n",
    "\n",
    "```bash\n",
    "./dev lint    # Runs flake8 link check against PEP8 standard\n",
    "./dev format  # Auto-formats code that is non PEP8-compliant\n",
    "```\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "To build the model container and re-deploy the notebook cluster simply run:\n",
    "\n",
    "```bash\n",
    "./stack up\n",
    "```\n",
    "\n",
    "To stop the cluster and clear resources run:\n",
    "\n",
    "```bash\n",
    "./stack down\n",
    "```\n",
    "\n",
    "Please note, you will need to have authenticated with GCP CLI and have k8s context referencing the right GKE cluster. \n",
    "For more details on this see below. \n",
    "\n",
    "### Cluster Details\n",
    "\n",
    "Notebooks are deployed as a standalone application using [JupyterHub](https://jupyter.org/hub).\n",
    "These notebooks allow users to interact with the giga models through an interactive dashboard and to visualize/plot the model outputs through a streamlined interfaces.\n",
    "\n",
    "[Helm](https://helm.sh/) is used to manage the deployment - find the existing jupyterhub helm chart [here](https://artifacthub.io/packages/helm/jupyterhub/jupyterhub).\n",
    "The deployment configuration for this chart can be found in `deployment/values/prod.yaml`.\n",
    "The following configurations are managed with a custom configuration:\n",
    "1. The base notebook container used in the deployment that includes the models\n",
    "2. The authentication mechanism for users to access jupyterhub - auth0 is currently used\n",
    "\n",
    "### Resource Requirements\n",
    "\n",
    "The model application is typically memory constrained rather than CPU constrained.\n",
    "The recommended minimum memory for a single model pod in k8s is 3 GB with a limit 5 GB.\n",
    "The current configuration is set to reflect this as follows (from deployment/help/prod.yaml):\n",
    "\n",
    "```\n",
    "singleuser:\n",
    "  # other configs ...\n",
    "  cpu:\n",
    "    limit:\n",
    "    guarantee:\n",
    "  memory:\n",
    "    limit: 5G\n",
    "    guarantee: 3G\n",
    "```\n",
    "\n",
    "Do note that no cpu limit is not set above.\n",
    "Additionally, the somewhat large memory guarantee is needed to run models for all schools in large countries (like Brazil).\n",
    "If the school data is broken down into smaller sub-regions for those larger countries, it's likely possible to make the memory guarantee significantly smaller.\n",
    "\n",
    "### Deployment Workflow\n",
    "Please note that the workflow is currently manually managed with the CLI explained below.\n",
    "The full deployment workflow looks as follows, which can all be managed with the `stack` CLI: \n",
    "1. Authenticate with GCP by running `./stack auth`. This will also configure the credentials for the GKE cluster to which jupyterhub is deployed\n",
    "2. Create a Docker image for the models, you can use the CLI in the root dir: `./stack create-image`\n",
    "3. Push the image to Actual's docker registry: `./stack push-image`\n",
    "4. Update or launch a new instance of the cluster with `./stack launch` \n",
    "\n",
    "### Updating the Cluster + Local Testing\n",
    "\n",
    "You can stop the jupyterhub cluster by running `./stack stop`.\n",
    "If you need to update the single user image, you can rebuild it using the CLI above.\n",
    "You can interact with the single user container locally by running `./stack start-container <local-workspace>`.\n",
    "\n",
    "### Authentication and Authorization Configuration\n",
    "\n",
    "You can configure the application cluster to use a number of different authenticators, these include:\n",
    "\n",
    "* github\n",
    "* Azure Active Directory\n",
    "* Auth0\n",
    "* Google auth\n",
    "\n",
    "You can read more about configuring each of these [here](https://z2jh.jupyter.org/en/stable/administrator/authentication.html).\n",
    "\n",
    "To configure an authenticator you will need to update the helm values in deployment/help/prod.yaml under\n",
    "\n",
    "```\n",
    "hub:\n",
    "  config:\n",
    "    # authenticator configuration here, for auth0 see example below\n",
    "    Auth0OAuthenticator:\n",
    "      client_id: client-id-from-auth0-here\n",
    "      client_secret: client-secret-from-auth0-here\n",
    "      oauth_callback_url: https://your-jupyterhub-domain/hub/oauth_callback\n",
    "      scope:\n",
    "        - openid\n",
    "        - email\n",
    "      auth0_subdomain: prod-8ua-1yy9\n",
    "    Authenticator:\n",
    "      admin_users:\n",
    "        - devops@example.com\n",
    "      auto_login: true\n",
    "    JupyterHub:\n",
    "      authenticator_class: auth0\n",
    "```\n",
    "\n",
    "### GCP and Auth0 Configurations\n",
    "\n",
    "Configuring the deployment is done in two places, the `stack` CLI and the deployment manifest of helm values.\n",
    "Most of the GCP specific deployment parameters are defined in the stack CLI, the ones of interest are the following:\n",
    "\n",
    "* The container registry, which is where all the built docker containers are pushed to and pulled from, see [here](stack#L6)\n",
    "* The cluster name, which points to the k8s cluster running the deployment, see [here](stack#L12)\n",
    "* The auth configuration is managed entirely inside of our deployment manifest, see [here](deployment/helm/prod.yaml#L31)\n",
    "\n",
    "Migrating to a different cloud provider or a different auth system would require updating these parameters.\n",
    "\n",
    "## CLI\n",
    "\n",
    "The library exposes the following CLI, each with a different purpose.\n",
    "\n",
    "For local development, the `./dev` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  build\t\t\t\t\tBuilds the modeling environment locally\n",
    "  start-notebook\t\t        Start a jupyterlab notebook server locally\n",
    "  test\t\t\t\t\tRuns the unit test suite\n",
    "  lint\t\t\t\t\tRuns a flake8 lint check against PEP 8\n",
    "  format\t\t\t\tModifies non PEP 8 compliant code to be style compliant\n",
    "  clean-notebook <notebook-path> \tRemoves rendered html from jupyter notebooks\n",
    "```\n",
    "\n",
    "For managing deployments, the `./stack` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  up \t\t\t\t\t\t        Deploys the notebook stack to a k8s cluster\n",
    "  down \t\t\t\t\t\t        Tears down the notebook stack\n",
    "  install \t\t\t\t\t        Install minikube, helm, etc.\n",
    "  auth \t\t\t\t\t\t        Authenticate with GCP\n",
    "  create-image \t\t\t\t\t        Builds docker image for off-platform models\n",
    "  push-image \t\t\t\t\t        Pushes model docker image to a remote registry\n",
    "  start-container <workspace-dir> \t                Launches a Docker container and mounts a workspace directory to it\n",
    "  launch  \t\t\t\t\t        Launches jupyterhub on a kubernetes cluster using helm\n",
    "  stop  \t\t\t\t\t        Stops the jupyterhub deployment\n",
    "  reset-password  <user-email> \t\t                Sends a password reset email for notebook user\n",
    "```\n",
    "\n",
    "For running the models and relevant data pipelines, the `./run` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  upload-workspace <workspace-dir> \t\t\tCopies the data workspace from the specified target directory to a storage bucket\n",
    "  fetch-workspace <workspace-dir> \t\t\tCopies the data workspace from a storage bucket to the specified target directory\n",
    "```\n",
    "\n",
    "#### (Optional) Execute scenarios directly with Python\n",
    "\n",
    "You can use the script below to run the total cost scenario by doing the following:\n",
    "\n",
    "```bash\n",
    "./total_cost_scenario.py --workspace <path-to-data-workspace>\n",
    "\t\t\t\t\t     --output-file <desired-output-file> # e.g. costs.csv\n",
    "\t\t\t\t\t     --scenario-type minimum-cost # minimum-cost, fiber, cellular, p2p, or satellite\n",
    "```\n",
    "\n",
    "The script above will use the school, fiber, and cellular data in the workspace specified, to create an output .csv table that contains cost information for each school in the input data set.\n",
    "Additionally, you can specify the scenario type by choosing between a `minimum-cost` scenario or a single technology cost scenario (`fiber`, `cellular`, `p2p`, `satellite`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
